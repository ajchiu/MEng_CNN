{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749177ff-3ed3-4933-af1a-e96497bd2301",
   "metadata": {},
   "source": [
    "TODO: test batch size other than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585941ef-25c1-463d-b168-39ce2b474ce1",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6061aa55-251b-4364-b330-988f655c4917",
   "metadata": {
    "id": "6061aa55-251b-4364-b330-988f655c4917",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import r2_score\n",
    "from BinvoxDataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75409c2a-7c7e-454a-b493-a8894b7469ce",
   "metadata": {
    "id": "75409c2a-7c7e-454a-b493-a8894b7469ce",
    "outputId": "124f3a64-fd41-43f6-8142-e776ca7c728a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6923a256-f4d8-4be9-8cd5-4151db71895e",
   "metadata": {
    "id": "6923a256-f4d8-4be9-8cd5-4151db71895e",
    "outputId": "d4ecf0c7-c798-41ef-9d25-e184eaf4aaf7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f84b18-6c7d-45e9-baa3-9bf5ea3b5149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Binvox Read Write\n",
    "#  Copyright (C) 2012 Daniel Maturana\n",
    "#  This file is part of binvox-rw-py.\n",
    "#\n",
    "#  binvox-rw-py is free software: you can redistribute it and/or modify\n",
    "#  it under the terms of the GNU General Public License as published by\n",
    "#  the Free Software Foundation, either version 3 of the License, or\n",
    "#  (at your option) any later version.\n",
    "#\n",
    "#  binvox-rw-py is distributed in the hope that it will be useful,\n",
    "#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#  GNU General Public License for more details.\n",
    "#\n",
    "#  You should have received a copy of the GNU General Public License\n",
    "#  along with binvox-rw-py. If not, see <http://www.gnu.org/licenses/>.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Voxels(object):\n",
    "    \"\"\" Holds a binvox model.\n",
    "    data is either a three-dimensional numpy boolean array (dense representation)\n",
    "    or a two-dimensional numpy float array (coordinate representation).\n",
    "\n",
    "    dims, translate and scale are the model metadata.\n",
    "\n",
    "    dims are the voxel dimensions, e.g. [32, 32, 32] for a 32x32x32 model.\n",
    "\n",
    "    scale and translate relate the voxels to the original model coordinates.\n",
    "\n",
    "    To translate voxel coordinates i, j, k to original coordinates x, y, z:\n",
    "\n",
    "    x_n = (i+.5)/dims[0]\n",
    "    y_n = (j+.5)/dims[1]\n",
    "    z_n = (k+.5)/dims[2]\n",
    "    x = scale*x_n + translate[0]\n",
    "    y = scale*y_n + translate[1]\n",
    "    z = scale*z_n + translate[2]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, dims, translate, scale, axis_order):\n",
    "        self.data = data\n",
    "        self.dims = dims\n",
    "        self.translate = translate\n",
    "        self.scale = scale\n",
    "        assert (axis_order in ('xzy', 'xyz'))\n",
    "        self.axis_order = axis_order\n",
    "\n",
    "    def clone(self):\n",
    "        data = self.data.copy()\n",
    "        dims = self.dims[:]\n",
    "        translate = self.translate[:]\n",
    "        return Voxels(data, dims, translate, self.scale, self.axis_order)\n",
    "\n",
    "    def write(self, fp):\n",
    "        write(self, fp)\n",
    "\n",
    "def read_header(fp):\n",
    "    \"\"\" Read binvox header. Mostly meant for internal use.\n",
    "    \"\"\"\n",
    "    line = fp.readline().strip()\n",
    "    if not line.startswith(b'#binvox'):\n",
    "        raise IOError('Not a binvox file')\n",
    "    dims = list(map(int, fp.readline().strip().split(b' ')[1:]))\n",
    "    translate = list(map(float, fp.readline().strip().split(b' ')[1:]))\n",
    "    scale = list(map(float, fp.readline().strip().split(b' ')[1:]))[0]\n",
    "    line = fp.readline()\n",
    "    return dims, translate, scale\n",
    "\n",
    "def read_as_3d_array(fp, fix_coords=True):\n",
    "    \"\"\" Read binary binvox format as array.\n",
    "\n",
    "    Returns the model with accompanying metadata.\n",
    "\n",
    "    Voxels are stored in a three-dimensional numpy array, which is simple and\n",
    "    direct, but may use a lot of memory for large models. (Storage requirements\n",
    "    are 8*(d^3) bytes, where d is the dimensions of the binvox model. Numpy\n",
    "    boolean arrays use a byte per element).\n",
    "\n",
    "    Doesn't do any checks on input except for the '#binvox' line.\n",
    "    \"\"\"\n",
    "    dims, translate, scale = read_header(fp)\n",
    "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
    "    # if just using reshape() on the raw data:\n",
    "    # indexing the array as array[i,j,k], the indices map into the\n",
    "    # coords as:\n",
    "    # i -> x\n",
    "    # j -> z\n",
    "    # k -> y\n",
    "    # if fix_coords is true, then data is rearranged so that\n",
    "    # mapping is\n",
    "    # i -> x\n",
    "    # j -> y\n",
    "    # k -> z\n",
    "    values, counts = raw_data[::2], raw_data[1::2]\n",
    "    data = np.repeat(values, counts).astype(bool)\n",
    "    data = data.reshape(dims)\n",
    "    if fix_coords:\n",
    "        # xzy to xyz TODO the right thing\n",
    "        data = np.transpose(data, (0, 2, 1))\n",
    "        axis_order = 'xyz'\n",
    "    else:\n",
    "        axis_order = 'xzy'\n",
    "    return Voxels(data, dims, translate, scale, axis_order)\n",
    "\n",
    "def read_as_coord_array(fp, fix_coords=True):\n",
    "    \"\"\" Read binary binvox format as coordinates.\n",
    "\n",
    "    Returns binvox model with voxels in a \"coordinate\" representation, i.e.  an\n",
    "    3 x N array where N is the number of nonzero voxels. Each column\n",
    "    corresponds to a nonzero voxel and the 3 rows are the (x, z, y) coordinates\n",
    "    of the voxel.  (The odd ordering is due to the way binvox format lays out\n",
    "    data).  Note that coordinates refer to the binvox voxels, without any\n",
    "    scaling or translation.\n",
    "\n",
    "    Use this to save memory if your model is very sparse (mostly empty).\n",
    "\n",
    "    Doesn't do any checks on input except for the '#binvox' line.\n",
    "    \"\"\"\n",
    "    dims, translate, scale = read_header(fp)\n",
    "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
    "\n",
    "    values, counts = raw_data[::2], raw_data[1::2]\n",
    "\n",
    "    sz = np.prod(dims)\n",
    "    index, end_index = 0, 0\n",
    "    end_indices = np.cumsum(counts)\n",
    "    indices = np.concatenate(([0], end_indices[:-1])).astype(end_indices.dtype)\n",
    "\n",
    "    values = values.astype(np.bool)\n",
    "    indices = indices[values]\n",
    "    end_indices = end_indices[values]\n",
    "\n",
    "    nz_voxels = []\n",
    "    for index, end_index in zip(indices, end_indices):\n",
    "        nz_voxels.extend(range(index, end_index))\n",
    "    nz_voxels = np.array(nz_voxels)\n",
    "    # TODO are these dims correct?\n",
    "    # according to docs,\n",
    "    # index = x * wxh + z * width + y; // wxh = width * height = d * d\n",
    "\n",
    "    x = nz_voxels / (dims[0]*dims[1])\n",
    "    zwpy = nz_voxels % (dims[0]*dims[1]) # z*w + y\n",
    "    z = zwpy / dims[0]\n",
    "    y = zwpy % dims[0]\n",
    "    if fix_coords:\n",
    "        data = np.vstack((x, y, z))\n",
    "        axis_order = 'xyz'\n",
    "    else:\n",
    "        data = np.vstack((x, z, y))\n",
    "        axis_order = 'xzy'\n",
    "\n",
    "    #return Voxels(data, dims, translate, scale, axis_order)\n",
    "    return Voxels(np.ascontiguousarray(data), dims, translate, scale, axis_order)\n",
    "\n",
    "def dense_to_sparse(voxel_data, dtype=int):\n",
    "    \"\"\" From dense representation to sparse (coordinate) representation.\n",
    "    No coordinate reordering.\n",
    "    \"\"\"\n",
    "    if voxel_data.ndim!=3:\n",
    "        raise ValueError('voxel_data is wrong shape; should be 3D array.')\n",
    "    return np.asarray(np.nonzero(voxel_data), dtype)\n",
    "\n",
    "def sparse_to_dense(voxel_data, dims, dtype=bool):\n",
    "    if voxel_data.ndim!=2 or voxel_data.shape[0]!=3:\n",
    "        raise ValueError('voxel_data is wrong shape; should be 3xN array.')\n",
    "    if np.isscalar(dims):\n",
    "        dims = [dims]*3\n",
    "    dims = np.atleast_2d(dims).T\n",
    "    # truncate to integers\n",
    "    xyz = voxel_data.astype(np.int)\n",
    "    # discard voxels that fall outside dims\n",
    "    valid_ix = ~np.any((xyz < 0) | (xyz >= dims), 0)\n",
    "    xyz = xyz[:,valid_ix]\n",
    "    out = np.zeros(dims.flatten(), dtype=dtype)\n",
    "    out[tuple(xyz)] = True\n",
    "    return out\n",
    "\n",
    "#def get_linear_index(x, y, z, dims):\n",
    "    #\"\"\" Assuming xzy order. (y increasing fastest.\n",
    "    #TODO ensure this is right when dims are not all same\n",
    "    #\"\"\"\n",
    "    #return x*(dims[1]*dims[2]) + z*dims[1] + y\n",
    "\n",
    "def write(voxel_model, fp):\n",
    "    \"\"\" Write binary binvox format.\n",
    "\n",
    "    Note that when saving a model in sparse (coordinate) format, it is first\n",
    "    converted to dense format.\n",
    "\n",
    "    Doesn't check if the model is 'sane'.\n",
    "\n",
    "    \"\"\"\n",
    "    if voxel_model.data.ndim==2:\n",
    "        # TODO avoid conversion to dense\n",
    "        dense_voxel_data = sparse_to_dense(voxel_model.data, voxel_model.dims)\n",
    "    else:\n",
    "        dense_voxel_data = voxel_model.data\n",
    "\n",
    "    fp.write('#binvox 1\\n')\n",
    "    fp.write('dim '+' '.join(map(str, voxel_model.dims))+'\\n')\n",
    "    fp.write('translate '+' '.join(map(str, voxel_model.translate))+'\\n')\n",
    "    fp.write('scale '+str(voxel_model.scale)+'\\n')\n",
    "    fp.write('data\\n')\n",
    "    if not voxel_model.axis_order in ('xzy', 'xyz'):\n",
    "        raise ValueError('Unsupported voxel model axis order')\n",
    "\n",
    "    if voxel_model.axis_order=='xzy':\n",
    "        voxels_flat = dense_voxel_data.flatten()\n",
    "    elif voxel_model.axis_order=='xyz':\n",
    "        voxels_flat = np.transpose(dense_voxel_data, (0, 2, 1)).flatten()\n",
    "\n",
    "    # keep a sort of state machine for writing run length encoding\n",
    "    state = voxels_flat[0]\n",
    "    ctr = 0\n",
    "    for c in voxels_flat:\n",
    "        if c==state:\n",
    "            ctr += 1\n",
    "            # if ctr hits max, dump\n",
    "            if ctr==255:\n",
    "                fp.write(chr(state))\n",
    "                fp.write(chr(ctr))\n",
    "                ctr = 0\n",
    "        else:\n",
    "            # if switch state, dump\n",
    "            fp.write(chr(state))\n",
    "            fp.write(chr(ctr))\n",
    "            state = c\n",
    "            ctr = 1\n",
    "    # flush out remainders\n",
    "    if ctr > 0:\n",
    "        fp.write(chr(state))\n",
    "        fp.write(chr(ctr))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import doctest\n",
    "    doctest.testmod()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca78b2-c4d7-42d8-9892-f9208ea51be7",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296f81a9-f655-43fd-b48e-2b321014569f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform(voxel):\n",
    "    return torch.unsqueeze(torch.tensor(voxel, dtype = torch.float32), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e2ce47-f930-4911-9852-d4aeeeac9e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_folder_path = '../AdditiveParts/data/parts_0, files 1 through 3950/Binvox_files_default_res/'\n",
    "label_file_path = '../AdditiveParts/data/Tweaker Orientation Score/Tweaker Orientation Score/parts0_1-3950.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0521cd03-b614-41ce-9fb7-93d1c40393ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(input_folder_path = input_folder_path, label_file_path = label_file_path, transform = transform, max_count = None, ram_limit = 1000)\n",
    "dataset_limited = CustomDataset(input_folder_path = input_folder_path, label_file_path = label_file_path, transform = transform, max_count = 5000, ram_limit = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f42434-93ee-4923-9809-4aed63ddcce2",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d2623ad-319a-4cb5-89cf-f279814a831e",
   "metadata": {
    "id": "8d2623ad-319a-4cb5-89cf-f279814a831e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, activation_fn = nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pooling_2 = nn.MaxPool3d(kernel_size = 2)\n",
    "\n",
    "        self.up_sampling_2 = nn.Upsample(scale_factor = 2)\n",
    "\n",
    "        self.conv64_1_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 1, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_8_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_32_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_128_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_128_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_256_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_384_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 384, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_160_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 160, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_40_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 40, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 1, kernel_size = kernel_size, padding = 'same'),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv64_1_8(x)\n",
    "        x = self.conv64_8_8(x)\n",
    "        feature_map_64 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv32_8_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        feature_map_32 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv16_32_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        feature_map_16 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv8_128_256(x)\n",
    "        x = self.conv8_256_256(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_16, x), dim = 1)\n",
    "        x = self.conv16_384_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_32, x), dim = 1)\n",
    "        x = self.conv32_160_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_64, x), dim = 1)\n",
    "        x = self.conv64_40_8(x)\n",
    "        x = self.conv64_8_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d0082e-99dc-49ac-aa28-b190cfc76d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNetScalarLabel(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, activation_fn = nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pooling_2 = nn.MaxPool3d(kernel_size = 2)\n",
    "        self.max_pooling_1 = nn.MaxPool3d(kernel_size = 1)\n",
    "\n",
    "        self.conv256_1_2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 1, out_channels = 2, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 2),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv256_2_2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 2, out_channels = 2, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 2),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv128_2_4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 2, out_channels = 4, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 4),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv128_4_4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 4, out_channels = 4, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 4),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_4_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 4, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_8_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_32_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_128_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_128_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_256_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv4_256_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 32, kernel_size = 3, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv4_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = 3, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv2_32_1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 1, kernel_size = 1, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 1),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv256_1_2(x)\n",
    "        x = self.conv256_2_2(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv128_2_4(x)\n",
    "        x = self.conv128_4_4(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv64_4_8(x)\n",
    "        x = self.conv64_8_8(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv32_8_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv16_32_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv8_128_256(x)\n",
    "        x = self.conv8_256_256(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv4_256_32(x)\n",
    "        x = self.conv4_32_32(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv2_32_1(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        # print('outcome shape', torch.squeeze(x).shape)\n",
    "        # return torch.reshape(x, (x.shape[0], 1))\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99138ba4-a9d8-4108-96da-6b503e54c7f1",
   "metadata": {},
   "source": [
    "# Define Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676a283b-9009-4ac8-8fd3-c1ef8b65c4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, training_loader, optimizer, loss_fn):\n",
    "    cumulative_loss = 0.0\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.squeeze(labels)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss and its gradients\n",
    "        # print('label shape', labels.shape)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        wandb.log({'batch loss': loss.item()})\n",
    "    return cumulative_loss / len(training_loader), cumulative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af6093e9-59db-4985-91a0-cf5d2bb9b3e7",
   "metadata": {
    "id": "af6093e9-59db-4985-91a0-cf5d2bb9b3e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(config, loss_fn):\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # initialize a wandb run\n",
    "    wandb.init(config = config)\n",
    "\n",
    "    # copy the config\n",
    "    config = wandb.config\n",
    "    \n",
    "    print('config:', config)\n",
    "\n",
    "    # get training loader\n",
    "    training_loader = DataLoader(dataset, batch_size = config.batch_size, shuffle = False)\n",
    "\n",
    "    # initialize model\n",
    "    if config.activation_fn == 'ReLU':\n",
    "        activation_fn = nn.ReLU()\n",
    "    \n",
    "    if config.activation_fn == 'Sigmoid':\n",
    "        activation_fn = nn.Sigmoid()\n",
    "    \n",
    "    model = ConvNetScalarLabel(kernel_size = config.kernel_size, activation_fn = activation_fn).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = config.learning_rate, momentum = 0.9)\n",
    "\n",
    "    for epoch in range(config.epochs_choice):\n",
    "        avg_loss_per_batch, cumulative_loss = train_epoch(model, training_loader, optimizer, loss_fn)\n",
    "        wandb.log({'avg_loss_per_batch': avg_loss_per_batch, 'cumulative_loss': cumulative_loss})\n",
    "        print(f'Loss for epoch {epoch}: {cumulative_loss}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77c5334-2ce1-49ea-8785-c499da625081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(config, model, loss_fn):\n",
    "    # copy the config\n",
    "    config = wandb.config\n",
    "    \n",
    "    # get testing loader\n",
    "    testing_loader = DataLoader(dataset_limited, batch_size = config.batch_size, shuffle = False)\n",
    "    \n",
    "    testing_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, data in enumerate(testing_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        testing_loss += loss.item()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy().tolist())\n",
    "        y_pred.extend(outputs.cpu().detach().numpy().tolist())\n",
    "    return testing_loss / len(testing_loader), testing_loss, r2_score(y_true = y_true, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45e5a4d-fe1f-4629-a456-1865f7e9906b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(config = None):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = train(config, loss_fn)\n",
    "    avg_loss_per_batch_test, testing_loss, r2 = test(config, model, loss_fn)\n",
    "    wandb.log({'avg_loss_per_batch_test': avg_loss_per_batch_test, 'testing_loss': testing_loss, 'r2': r2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3855e-1af6-4dac-a2eb-9afca254bdcd",
   "metadata": {},
   "source": [
    "# Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2403886e-355f-4658-8199-aebb835516bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "metric = {\n",
    "    'name': 'testing_loss',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'kernel_size': {\n",
    "        'values': [3, 4, 5]\n",
    "    },\n",
    "    'activation_fn': {\n",
    "        'values': ['ReLU', 'Sigmoid']\n",
    "    },\n",
    "    'epochs_choice': {\n",
    "          'values': [5, 10, 20]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'values': [1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [8, 4]\n",
    "    },\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    'kernel_size': {\n",
    "        'values': [3]\n",
    "    },\n",
    "    'activation_fn': {\n",
    "        'values': ['ReLU']\n",
    "    },\n",
    "    'epochs_choice': {\n",
    "          'values': [5]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'values': [1e-3]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [4]\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83411fe0-67e5-49c3-b22a-9d5ac1f5abd1",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f5fe4f-57a0-4445-aaec-58183fb12613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: smc1tzhb\n",
      "Sweep URL: https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/smc1tzhb\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project = 'CNN_sweep_scalar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fec8f8da-e378-4181-8bcc-fc83a0a26754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'activation_fn': 'ReLU', 'batch_size': 4, 'epochs_choice': 5, 'kernel_size': 3, 'learning_rate': 0.001}\n",
      "Loading samples 0 through 999\n",
      "Processing sample number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id = sweep_id, function = evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d71f0e-e630-4491-85ad-6182de3cd2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
