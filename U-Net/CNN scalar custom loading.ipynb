{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749177ff-3ed3-4933-af1a-e96497bd2301",
   "metadata": {},
   "source": [
    "TODO: test batch size other than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585941ef-25c1-463d-b168-39ce2b474ce1",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6061aa55-251b-4364-b330-988f655c4917",
   "metadata": {
    "id": "6061aa55-251b-4364-b330-988f655c4917",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import r2_score\n",
    "from BinvoxDataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75409c2a-7c7e-454a-b493-a8894b7469ce",
   "metadata": {
    "id": "75409c2a-7c7e-454a-b493-a8894b7469ce",
    "outputId": "124f3a64-fd41-43f6-8142-e776ca7c728a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6923a256-f4d8-4be9-8cd5-4151db71895e",
   "metadata": {
    "id": "6923a256-f4d8-4be9-8cd5-4151db71895e",
    "outputId": "d4ecf0c7-c798-41ef-9d25-e184eaf4aaf7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchangli_824\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca78b2-c4d7-42d8-9892-f9208ea51be7",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296f81a9-f655-43fd-b48e-2b321014569f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform(voxel):\n",
    "    return torch.unsqueeze(torch.tensor(voxel, dtype = torch.float32), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e2ce47-f930-4911-9852-d4aeeeac9e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "paths = json.load(open('config.json', 'r'))\n",
    "input_folder_path = paths['input_folder_path']\n",
    "label_file_path = paths['label_file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0521cd03-b614-41ce-9fb7-93d1c40393ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(input_folder_path = input_folder_path, label_file_path = label_file_path, transform = transform, max_count = None, ram_limit = 1000)\n",
    "dataset_limited = CustomDataset(input_folder_path = input_folder_path, label_file_path = label_file_path, transform = transform, max_count = 5000, ram_limit = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f42434-93ee-4923-9809-4aed63ddcce2",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2623ad-319a-4cb5-89cf-f279814a831e",
   "metadata": {
    "id": "8d2623ad-319a-4cb5-89cf-f279814a831e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, activation_fn = nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pooling_2 = nn.MaxPool3d(kernel_size = 2)\n",
    "\n",
    "        self.up_sampling_2 = nn.Upsample(scale_factor = 2)\n",
    "\n",
    "        self.conv64_1_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 1, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_8_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_32_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_128_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_128_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_256_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_384_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 384, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_160_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 160, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_40_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 40, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 1, kernel_size = kernel_size, padding = 'same'),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv64_1_8(x)\n",
    "        x = self.conv64_8_8(x)\n",
    "        feature_map_64 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv32_8_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        feature_map_32 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv16_32_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        feature_map_16 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv8_128_256(x)\n",
    "        x = self.conv8_256_256(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_16, x), dim = 1)\n",
    "        x = self.conv16_384_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_32, x), dim = 1)\n",
    "        x = self.conv32_160_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_64, x), dim = 1)\n",
    "        x = self.conv64_40_8(x)\n",
    "        x = self.conv64_8_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d0082e-99dc-49ac-aa28-b190cfc76d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNetScalarLabel(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, activation_fn = nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pooling_2 = nn.MaxPool3d(kernel_size = 2)\n",
    "        self.max_pooling_1 = nn.MaxPool3d(kernel_size = 1)\n",
    "        self.max_pooling_8 = nn.MaxPool3d(kernel_size = 8)\n",
    "        self.linear_1 = nn.Linear(256, 16)\n",
    "        self.linear_2 = nn.Linear(16, 1)\n",
    "\n",
    "        self.conv256_1_2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 1, out_channels = 2, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 2),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv256_2_2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 2, out_channels = 2, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 2),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv128_2_4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 2, out_channels = 4, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 4),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv128_4_4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 4, out_channels = 4, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 4),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_1_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 1, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_4_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 4, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_8_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_32_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_128_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_128_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_256_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv4_256_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 32, kernel_size = 3, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv4_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = 3, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv2_32_1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 1, kernel_size = 1, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 1),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv256_1_2(x)\n",
    "        x = self.conv256_2_2(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv128_2_4(x)\n",
    "        x = self.conv128_4_4(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv64_4_8(x)\n",
    "        x = self.conv64_8_8(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv32_8_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv16_32_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv8_128_256(x)\n",
    "        x = self.conv8_256_256(x)\n",
    "        x = self.max_pooling_8(x)\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        # print('outcome shape', torch.squeeze(x).shape)\n",
    "        # return torch.reshape(x, (x.shape[0], 1))\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99138ba4-a9d8-4108-96da-6b503e54c7f1",
   "metadata": {},
   "source": [
    "# Define Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "676a283b-9009-4ac8-8fd3-c1ef8b65c4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, training_loader, optimizer, loss_fn):\n",
    "    cumulative_loss = 0.0\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.squeeze(labels)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss and its gradients\n",
    "        # print('label shape', labels.shape)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        wandb.log({'batch loss': loss.item()})\n",
    "    return cumulative_loss / len(training_loader), cumulative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6093e9-59db-4985-91a0-cf5d2bb9b3e7",
   "metadata": {
    "id": "af6093e9-59db-4985-91a0-cf5d2bb9b3e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(config, loss_fn):\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # initialize a wandb run\n",
    "    wandb.init(config = config)\n",
    "\n",
    "    # copy the config\n",
    "    config = wandb.config\n",
    "    \n",
    "    print('config:', config)\n",
    "\n",
    "    # get training loader\n",
    "    training_loader = DataLoader(dataset, batch_size = config.batch_size, shuffle = False)\n",
    "\n",
    "    # initialize model\n",
    "    if config.activation_fn == 'ReLU':\n",
    "        activation_fn = nn.ReLU()\n",
    "    \n",
    "    if config.activation_fn == 'Sigmoid':\n",
    "        activation_fn = nn.Sigmoid()\n",
    "    \n",
    "    model = ConvNetScalarLabel(kernel_size = config.kernel_size, activation_fn = activation_fn).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = config.learning_rate, momentum = 0.9)\n",
    "\n",
    "    for epoch in range(config.epochs_choice):\n",
    "        avg_loss_per_batch, cumulative_loss = train_epoch(model, training_loader, optimizer, loss_fn)\n",
    "        wandb.log({'avg_loss_per_batch': avg_loss_per_batch, 'cumulative_loss': cumulative_loss})\n",
    "        print(f'Loss for epoch {epoch}: {cumulative_loss}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77c5334-2ce1-49ea-8785-c499da625081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(config, model, loss_fn):\n",
    "    # copy the config\n",
    "    config = wandb.config\n",
    "    \n",
    "    # get testing loader\n",
    "    testing_loader = DataLoader(dataset_limited, batch_size = config.batch_size, shuffle = False)\n",
    "    \n",
    "    testing_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, data in enumerate(testing_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        testing_loss += loss.item()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy().tolist())\n",
    "        y_pred.extend(outputs.cpu().detach().numpy().tolist())\n",
    "    return testing_loss / len(testing_loader), testing_loss, r2_score(y_true = y_true, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45e5a4d-fe1f-4629-a456-1865f7e9906b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(config = None):\n",
    "    loss_fn = nn.L1Loss()\n",
    "    model = train(config, loss_fn)\n",
    "    avg_loss_per_batch_test, testing_loss, r2 = test(config, model, loss_fn)\n",
    "    wandb.log({'avg_loss_per_batch_test': avg_loss_per_batch_test, 'testing_loss': testing_loss, 'r2': r2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3855e-1af6-4dac-a2eb-9afca254bdcd",
   "metadata": {},
   "source": [
    "# Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2403886e-355f-4658-8199-aebb835516bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "metric = {\n",
    "    'name': 'testing_loss',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'kernel_size': {\n",
    "        'values': [3, 4, 5]\n",
    "    },\n",
    "    'activation_fn': {\n",
    "        'values': ['ReLU', 'Sigmoid']\n",
    "    },\n",
    "    'epochs_choice': {\n",
    "          'values': [5, 10, 20]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'values': [1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [8, 4]\n",
    "    },\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    'kernel_size': {\n",
    "        'values': [3]\n",
    "    },\n",
    "    'activation_fn': {\n",
    "        'values': ['ReLU']\n",
    "    },\n",
    "    'epochs_choice': {\n",
    "          'values': [5]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'values': [1e-3]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [4]\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83411fe0-67e5-49c3-b22a-9d5ac1f5abd1",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f5fe4f-57a0-4445-aaec-58183fb12613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: r3fuqrh4\n",
      "Sweep URL: https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/r3fuqrh4\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project = 'CNN_sweep_scalar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec8f8da-e378-4181-8bcc-fc83a0a26754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\silly bb\\Desktop\\Capstone\\CNN\\U-Net\\wandb\\run-20240129_215325-9mhetjbd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/changli_824/CNN_sweep_scalar/runs/9mhetjbd' target=\"_blank\">magic-sweep-1</a></strong> to <a href='https://wandb.ai/changli_824/CNN_sweep_scalar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/r3fuqrh4' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/r3fuqrh4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/changli_824/CNN_sweep_scalar' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/r3fuqrh4' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/r3fuqrh4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/runs/9mhetjbd' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/runs/9mhetjbd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'activation_fn': 'ReLU', 'batch_size': 4, 'epochs_choice': 5, 'kernel_size': 3, 'learning_rate': 0.001}\n",
      "Loading samples 0 through 999\n",
      "Processing sample number 0\n",
      "Loading samples 1000 through 1999\n",
      "Processing sample number 1000\n",
      "Loading samples 2000 through 2999\n",
      "Processing sample number 2000\n",
      "Loading samples 3000 through 3999\n",
      "Processing sample number 3000\n",
      "Loading samples 4000 through 4999\n",
      "Processing sample number 4000\n",
      "Loading samples 5000 through 5999\n",
      "Processing sample number 5000\n",
      "Loading samples 6000 through 6999\n",
      "Processing sample number 6000\n",
      "Loading samples 7000 through 7999\n",
      "Processing sample number 7000\n",
      "Loading samples 8000 through 8999\n",
      "Processing sample number 8000\n",
      "Loading samples 9000 through 9999\n",
      "Processing sample number 9000\n",
      "Loading samples 10000 through 10999\n",
      "Processing sample number 10000\n",
      "Loading samples 11000 through 11999\n",
      "Processing sample number 11000\n",
      "Loading samples 12000 through 12999\n",
      "Processing sample number 12000\n",
      "Loading samples 13000 through 13999\n",
      "Processing sample number 13000\n",
      "Loading samples 14000 through 14999\n",
      "Processing sample number 14000\n",
      "Loading samples 15000 through 15999\n",
      "Processing sample number 15000\n",
      "Loading samples 16000 through 16999\n",
      "Processing sample number 16000\n",
      "Loading samples 17000 through 17999\n",
      "Processing sample number 17000\n",
      "Loading samples 18000 through 18999\n",
      "Processing sample number 18000\n",
      "Loading samples 19000 through 19999\n",
      "Processing sample number 19000\n",
      "Loading samples 20000 through 20999\n",
      "Processing sample number 20000\n",
      "Loading samples 21000 through 21999\n",
      "Processing sample number 21000\n",
      "Loading samples 22000 through 22721\n",
      "Processing sample number 22000\n",
      "Loss for epoch 0: 22590.6947658509\n",
      "Loading samples 0 through 999\n",
      "Processing sample number 0\n",
      "Loading samples 1000 through 1999\n",
      "Processing sample number 1000\n",
      "Loading samples 2000 through 2999\n",
      "Processing sample number 2000\n",
      "Loading samples 3000 through 3999\n",
      "Processing sample number 3000\n",
      "Loading samples 4000 through 4999\n",
      "Processing sample number 4000\n",
      "Loading samples 5000 through 5999\n",
      "Processing sample number 5000\n",
      "Loading samples 6000 through 6999\n",
      "Processing sample number 6000\n",
      "Loading samples 7000 through 7999\n",
      "Processing sample number 7000\n",
      "Loading samples 8000 through 8999\n",
      "Processing sample number 8000\n",
      "Loading samples 9000 through 9999\n",
      "Processing sample number 9000\n",
      "Loading samples 10000 through 10999\n",
      "Processing sample number 10000\n",
      "Loading samples 11000 through 11999\n",
      "Processing sample number 11000\n",
      "Loading samples 12000 through 12999\n",
      "Processing sample number 12000\n",
      "Loading samples 13000 through 13999\n",
      "Processing sample number 13000\n",
      "Loading samples 14000 through 14999\n",
      "Processing sample number 14000\n",
      "Loading samples 15000 through 15999\n",
      "Processing sample number 15000\n",
      "Loading samples 16000 through 16999\n",
      "Processing sample number 16000\n",
      "Loading samples 17000 through 17999\n",
      "Processing sample number 17000\n",
      "Loading samples 18000 through 18999\n",
      "Processing sample number 18000\n",
      "Loading samples 19000 through 19999\n",
      "Processing sample number 19000\n",
      "Loading samples 20000 through 20999\n",
      "Processing sample number 20000\n",
      "Loading samples 21000 through 21999\n",
      "Processing sample number 21000\n",
      "Loading samples 22000 through 22721\n",
      "Processing sample number 22000\n",
      "Loss for epoch 1: 21748.359036508948\n",
      "Loading samples 0 through 999\n",
      "Processing sample number 0\n",
      "Loading samples 1000 through 1999\n",
      "Processing sample number 1000\n",
      "Loading samples 2000 through 2999\n",
      "Processing sample number 2000\n",
      "Loading samples 3000 through 3999\n",
      "Processing sample number 3000\n",
      "Loading samples 4000 through 4999\n",
      "Processing sample number 4000\n",
      "Loading samples 5000 through 5999\n",
      "Processing sample number 5000\n",
      "Loading samples 6000 through 6999\n",
      "Processing sample number 6000\n",
      "Loading samples 7000 through 7999\n",
      "Processing sample number 7000\n",
      "Loading samples 8000 through 8999\n",
      "Processing sample number 8000\n",
      "Loading samples 9000 through 9999\n",
      "Processing sample number 9000\n",
      "Loading samples 10000 through 10999\n",
      "Processing sample number 10000\n",
      "Loading samples 11000 through 11999\n",
      "Processing sample number 11000\n",
      "Loading samples 12000 through 12999\n",
      "Processing sample number 12000\n",
      "Loading samples 13000 through 13999\n",
      "Processing sample number 13000\n",
      "Loading samples 14000 through 14999\n",
      "Processing sample number 14000\n",
      "Loading samples 15000 through 15999\n",
      "Processing sample number 15000\n",
      "Loading samples 16000 through 16999\n",
      "Processing sample number 16000\n",
      "Loading samples 17000 through 17999\n",
      "Processing sample number 17000\n",
      "Loading samples 18000 through 18999\n",
      "Processing sample number 18000\n",
      "Loading samples 19000 through 19999\n",
      "Processing sample number 19000\n",
      "Loading samples 20000 through 20999\n",
      "Processing sample number 20000\n",
      "Loading samples 21000 through 21999\n",
      "Processing sample number 21000\n",
      "Loading samples 22000 through 22721\n",
      "Processing sample number 22000\n",
      "Loss for epoch 2: 21520.175000626594\n",
      "Loading samples 0 through 999\n",
      "Processing sample number 0\n",
      "Loading samples 1000 through 1999\n",
      "Processing sample number 1000\n",
      "Loading samples 2000 through 2999\n",
      "Processing sample number 2000\n",
      "Loading samples 3000 through 3999\n",
      "Processing sample number 3000\n",
      "Loading samples 4000 through 4999\n",
      "Processing sample number 4000\n",
      "Loading samples 5000 through 5999\n",
      "Processing sample number 5000\n",
      "Loading samples 6000 through 6999\n",
      "Processing sample number 6000\n",
      "Loading samples 7000 through 7999\n",
      "Processing sample number 7000\n",
      "Loading samples 8000 through 8999\n",
      "Processing sample number 8000\n",
      "Loading samples 9000 through 9999\n",
      "Processing sample number 9000\n",
      "Loading samples 10000 through 10999\n",
      "Processing sample number 10000\n",
      "Loading samples 11000 through 11999\n",
      "Processing sample number 11000\n",
      "Loading samples 12000 through 12999\n",
      "Processing sample number 12000\n",
      "Loading samples 13000 through 13999\n",
      "Processing sample number 13000\n",
      "Loading samples 14000 through 14999\n",
      "Processing sample number 14000\n",
      "Loading samples 15000 through 15999\n",
      "Processing sample number 15000\n",
      "Loading samples 16000 through 16999\n",
      "Processing sample number 16000\n",
      "Loading samples 17000 through 17999\n",
      "Processing sample number 17000\n",
      "Loading samples 18000 through 18999\n",
      "Processing sample number 18000\n",
      "Loading samples 19000 through 19999\n",
      "Processing sample number 19000\n",
      "Loading samples 20000 through 20999\n",
      "Processing sample number 20000\n",
      "Loading samples 21000 through 21999\n",
      "Processing sample number 21000\n",
      "Loading samples 22000 through 22721\n",
      "Processing sample number 22000\n",
      "Loss for epoch 3: 21362.87190836668\n",
      "Loading samples 0 through 999\n",
      "Processing sample number 0\n",
      "Loading samples 1000 through 1999\n",
      "Processing sample number 1000\n",
      "Loading samples 2000 through 2999\n",
      "Processing sample number 2000\n",
      "Loading samples 3000 through 3999\n",
      "Processing sample number 3000\n",
      "Loading samples 4000 through 4999\n",
      "Processing sample number 4000\n",
      "Loading samples 5000 through 5999\n",
      "Processing sample number 5000\n",
      "Loading samples 6000 through 6999\n",
      "Processing sample number 6000\n",
      "Loading samples 7000 through 7999\n",
      "Processing sample number 7000\n",
      "Loading samples 8000 through 8999\n",
      "Processing sample number 8000\n",
      "Loading samples 9000 through 9999\n",
      "Processing sample number 9000\n",
      "Loading samples 10000 through 10999\n",
      "Processing sample number 10000\n",
      "Loading samples 11000 through 11999\n",
      "Processing sample number 11000\n",
      "Loading samples 12000 through 12999\n",
      "Processing sample number 12000\n",
      "Loading samples 13000 through 13999\n",
      "Processing sample number 13000\n",
      "Loading samples 14000 through 14999\n",
      "Processing sample number 14000\n",
      "Loading samples 15000 through 15999\n",
      "Processing sample number 15000\n",
      "Loading samples 16000 through 16999\n",
      "Processing sample number 16000\n",
      "Loading samples 17000 through 17999\n",
      "Processing sample number 17000\n",
      "Loading samples 18000 through 18999\n",
      "Processing sample number 18000\n",
      "Loading samples 19000 through 19999\n",
      "Processing sample number 19000\n",
      "Loading samples 20000 through 20999\n",
      "Processing sample number 20000\n",
      "Loading samples 21000 through 21999\n",
      "Processing sample number 21000\n",
      "Loading samples 22000 through 22721\n",
      "Processing sample number 22000\n",
      "Loss for epoch 4: 21245.20132933557\n",
      "Loading samples 0 through 999\n",
      "Processing sample number 0\n",
      "Loading samples 1000 through 1999\n",
      "Processing sample number 1000\n",
      "Loading samples 2000 through 2999\n",
      "Processing sample number 2000\n",
      "Loading samples 3000 through 3999\n",
      "Processing sample number 3000\n",
      "Loading samples 4000 through 4999\n",
      "Processing sample number 4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss_per_batch</td><td>█▄▂▂▁</td></tr><tr><td>avg_loss_per_batch_test</td><td>▁</td></tr><tr><td>batch loss</td><td>▁▁▂▁▁▁▁▁▂▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▂▁▁▁▁▁▂▁▁▁</td></tr><tr><td>cumulative_loss</td><td>█▄▂▂▁</td></tr><tr><td>r2</td><td>▁</td></tr><tr><td>testing_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss_per_batch</td><td>3.73969</td></tr><tr><td>avg_loss_per_batch_test</td><td>3.12511</td></tr><tr><td>batch loss</td><td>0.26943</td></tr><tr><td>cumulative_loss</td><td>21245.20133</td></tr><tr><td>r2</td><td>-0.02955</td></tr><tr><td>testing_loss</td><td>3906.39121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sweep-1</strong> at: <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/runs/9mhetjbd' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/runs/9mhetjbd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240129_215325-9mhetjbd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id = sweep_id, function = evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d71f0e-e630-4491-85ad-6182de3cd2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
